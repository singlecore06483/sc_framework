import requests

def scan_robots_txt(url):
    try:
        response = requests.get(url + '/robots.txt')
        if response.status_code == 200:
            robots_content = response.text
            if 'Disallow' in robots_content:
                disallowed_entries = [entry.strip() for entry in robots_content.split('Disallow:')[1:]]
                print(f"Disallowed entries found in {url}/robots.txt:")
                for entry in disallowed_entries:
                    print(f"- {entry}")
            else:
                print(f"No disallowed entries found in {url}/robots.txt")
        else:
            print(f"Failed to retrieve robots.txt from {url}")
    except requests.exceptions.RequestException as e:
        print(f"Error occurred while scanning {url}: {e}")

# Example usage
url = input("URL: ")
scan_robots_txt(url)
